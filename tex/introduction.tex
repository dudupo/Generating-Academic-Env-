Coding theory has emerged by the need to transfer information in noisy communication channels. By embedding a message in higher dimension space, one can guarantee robustness against possible faults. The ratio of the original content length to the passed message \emph{length} is the \emph{rate} of the code, and it measures how consuming our communication protocol is. Furthermore, the \emph{distance} of the code quantifies how many faults the scheme can absorb such that the receiver can recover the original message. We could consider the code as all the strings that satisfy a specified restrictions collection.
  

  Non-formally, code is good if its distance and rate are scaled linearly in the encoded message length. In practice, one is also interested in implementing those checks efficiently. We say that a code is an LDPC if any bit is involved in a constant number of restrictions, each of which is a linear equation, and if any restriction contains a fixed number of variables.

  Furthermore, finally, another characteristic of the code is its testability, which is the complexity of the number of random checks one should do to negate that a given candidate is in the code. Besides good codes being considered efficient in terms of robustness and overhead, they are also vital components in establishing secure multiparty computation \cite{MultiParty} and have a deep connection to probabilistic proofs.

  First, we state the notations, definitions, and formal theorem in section 2. Then in sections 3 and 4, we review past results and provide their proofs to make this paper self-contained. Readers familiar with the basic concepts of LDPC, Tanner, and Expanders codes construction should consider skipping directly to section 5, in which we provide our proof. 


